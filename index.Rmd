---
title: "Analisis Tasa de Intervencion"
author: "Gabriel Orozco/Diana Aguirre/Edgard Camacho"
date: "2024-10-21"
output: document.html
---

# **Enunciado**

En este momento deberemos retomar la Unidad 1 en la cual se creó un minilibro que contiene el entregable de dicha unidad. Este documento tiene como repositorio GitHub (elaborado desde Markdown). Ahora, en esta Unidad 2, se debe continuar con los datos presentados en dicho entregable y se debe evidenciar, en una de las variables en el tiempo, la aproximación en promedio móvil, en rezagos y en estacionalidad. Todo lo anterior, a través de funciones y gráficas que permitan detectar patrones y ciclos de la variable.

## Análisis exploratorio

```{r echo=FALSE, message=FALSE}
#install.packages("readxl")
#install.packages("forecast")
#install.packages("timsac")
#install.packages("changepoint")
#install.packages("kableExtra")
#install.packages("bookdown")
#install.packages("xfun")
#install.packages("bookdown")
#install.packages("fabletools")
#install.packages("Rcpp")
#install.packages("prophet")
#install.packages("robust")
#install.packages("tsoutliers")
#install.packages("lubridate")
#install.packages("RSNNS")
#install.packages("nnfor")




library(TSA)
library(kableExtra)
library(tidyr)
library(zoo)
library(readxl)
library(forecast) # Recomendada profesora
library(ggplot2)
library(tseries) # Recomendada profesora
library(timsac)
library(changepoint)
library(ggplot2)
library(dplyr)
library(lubridate)
library(stats)
library(bookdown)
library(fabletools)
library(Rcpp)
library(prophet)
library(robust)
library(tsoutliers)
library(nnfor)
library(RSNNS)
library(prophet)
library(lubridate)
library(forecast)# NUEVA POR BOX Jenkins
library(tsoutliers)# NUEVA POR BOX Jenkins
library(timsac) # NUEVA POR BOX Jenkins
library(TSA)
library(kableExtra)
library(tidyr)
library(zoo)
library(readxl)
library(forecast) # Recomendada profesora
library(ggplot2)
library(tseries) # Recomendada profesora
library(timsac)
library(changepoint)
library(ggplot2)
library(dplyr)
library(lubridate)
library(stats)


```


```{r echo=FALSE}
data <- read_excel("1.2.TIP_Serie historica diaria.xlsx")
```

```{r echo=FALSE, warning=FALSE}
str(data)
```

Los datos representan una serie de tiempo de 310 filas y 2 columnas, correspondientes a la fecha y a la tasa. Se observa que la fecha realmente corresponde a un dato mensual por tanto conviene ajustar el formato.


```{r echo=FALSE, warning=FALSE}
# Primer y último registro del dataset
resultado <- rbind(head(data, 1), tail(data, 1))
print(resultado)

```
Al consultar el primer y último registro del dataset, se identifica que la observación más reciente corresponde al mes de octubre de 2024 con una tasa de 10.25%, mientras que el registro más antiguo es de enero de 1999, con una tasa de 26%. Estos datos indican que el dataset abarca un periodo de aproximadamente 25 años (310 meses), desde finales del siglo XX hasta la fecha actual, reflejando un amplio intervalo temporal que podría incluir distintas tendencias o cambios económicos en la variable Tasa.

```{r echo=FALSE, warning=FALSE}
summary(data)
```

* La serie cubre un rango de 25 años, con la mediana alrededor de 2011, lo que sugiere que los datos están relativamente bien distribuidos a lo largo del tiempo.
*  La tasa tiene una amplia variabilidad, con un valor mínimo de 1.75 y un máximo de 26. La mayor parte de los valores se concentran entre 4.25 y 9.25 (entre el primer y tercer cuartil). 

No se identifican datos ausentes:

```{r echo=FALSE, warning=FALSE}
n_nas_por_columna <- colSums(is.na(data))
print(n_nas_por_columna)
```




```{r echo=FALSE, warning=FALSE}
#Se realiza una copia del dataset por seguridad.
data2 <- data

```

Con el código siguiente, se agregan dos columnas adicionales, llamadas **Año** y **Mes**, lo anterior para poder tener una mejor visual de los datos, teniendo en cuenta el gran número de registros que tiene el dataset.


```{r echo=FALSE, warning=FALSE}


data2 <- data2 %>%
  mutate(Fecha = as.Date(Fecha), Anio = year(Fecha), Mes = month(Fecha, label = TRUE, abbr = TRUE)) %>%
  select(Anio, Mes, everything())

head(data2)



```

## Gráficos de visualización


```{r echo=FALSE, warning=FALSE}

ggplot(data2, aes(x = Fecha, y = Tasa)) +
  geom_point(color = "blue", size = 2) + 
  geom_smooth(method = "loess", color = "red", se = FALSE) +  
  labs(title = "Tasa a lo largo del tiempo",
       x = "Tiempo",
       y = "Tasa") +
  scale_x_date(date_labels = "%Y-%m", date_breaks = "1 month") +
  theme_minimal(base_size = 15) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    axis.title = element_text(face = "bold"),
    panel.grid.major = element_line(color = "lightgrey"),
    panel.grid.minor = element_blank()
  )


```



* **Puntos Azules:** Los puntos azules indican los valores de "Tasa" en momentos específicos. Hay una dispersión considerable, sugiriendo que la tasa ha experimentado fluctuaciones a lo largo del tiempo.

* **Línea Roja:** Esta línea es el resultado de un ajuste de suavización. La línea roja ilustra la tendencia general de la "Tasa" a lo largo del tiempo. A partir de la línea, se puede observar que, aunque hay variaciones, existe una tendencia que se puede analizar para hacer predicciones o entender mejor el comportamiento de la variable.

* **Variaciones:** La gráfica muestra que la Tasa ha tenido picos y valles, lo que podría indicar variaciones estacionales o influencias externas que afectan la variable a lo largo del tiempo. A partir de la línea de suavización, parece que la Tasa ha ido disminuyendo o estabilizándose en ciertos períodos, teniendo una caída significativa desde 1999 hasta 2003,  y un aumento importante de 2021 a 2024.



```{r echo=FALSE, warning=FALSE}

# 3. Gráfico de Puntos y Línea de Tendencia

ggplot(data2, aes(x = Anio, y = Tasa)) +
  geom_point(color = "blue", size = 2) +  # Puntos en azul
  geom_line(color = "blue", size = 1) +  # Línea azul
  geom_smooth(method = "loess", color = "red", se = FALSE) +  # Tendencia en rojo
  labs(title = "Tasa a lo largo del tiempo",
       x = "Anio",
       y = "Tasa") +
  scale_x_continuous(breaks = seq(min(data2$Anio), max(data2$Anio), by = 2)) +  # Eje X de 2 en 2
  scale_y_continuous(breaks = seq(min(data2$Tasa, na.rm = TRUE), max(data2$Tasa, na.rm = TRUE), by = 2)) +  # Eje Y de 2 en 2
  theme_minimal(base_size = 15) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    axis.title = element_text(face = "bold"),
    panel.grid.major = element_line(color = "lightgrey")
  )

```


  
  
Esta gráfica permite ver con más detalle los cambios de tendencia:

* Desde 1999, se confirma la disminución significativa en la tasa, que empieza muy alta (cerca de 25) y cae rápidamente hasta estabilizarse alrededor de los años 2007-2008 en un valor muy inferior (por debajo de 10).

* Entre 2005 y 2020, se observan picos y caídas a intervalos relativamente regulares, pero sin grandes cambios en los niveles generales hasta el repunte final.

* Eentre 2010 y 2020, la tasa se mantiene más estable, con algunas oscilaciones en torno a los 5-10 puntos.

* A partir de 2021, hay una tendencia de aumento, que se hace más pronunciada hacia los años más recientes.  Esto podría ser consecuencia de algún cambio en las políticas o factores externos como la pandemia de COVID-19.

* La línea de tendencia suavizada indica una caída rápida, seguida de un periodo de estabilización, y finalmente una tendencia de aumento en los años recientes, semejando una forma de "U" suavizada.



```{r echo=FALSE, warning=FALSE}

# Crear la tabla de tasas por Año y Mes
tabla_tasas <- data2 %>%
  group_by(Anio, Mes) %>%
  summarise(TasaPromedio = round(mean(Tasa, na.rm = TRUE), 1)) %>%  
  pivot_wider(names_from = Mes, values_from = TasaPromedio)  

# Mostrar la tabla en cuadrícula
kable(tabla_tasas, format = "html") %>%
  kable_styling(full_width = F, position = "left")


```

 A partir de la tabla se pueden confirmar las tendencias mencionadas anteriormente, con un comportamiento descendente en los 12 primeros años (1999 a 2010), una estabilización en los 5 años siguientes (2011-2015) y un incremento significativo en años recientes (2021 en adelante).



```{r echo=FALSE}

plot(data2$Anio, data2$Tasa, main = "Grafico del dataset", xlab = "Tiempo", ylab = "Tasa")

```
  
El **gráfico de dispersión temporal**, presenta una tendencia decreciente al inicio (antes de 2005), seguida por un período de estabilización y una ligera recuperación hacia 2020. Después de 2020, la tasa muestra un aumento significativo. La caída inicial y el posterior aumento alrededor de 2020 son destacables. 


## Análisis de serie de tiempo

## Promedio o media móvil


Permite analizar el mercado a través de las tendencias. La media móvil es una técnica estadística que se utiliza para analizar datos a lo largo del tiempo. Permite calcular la media de un conjunto de valores en un intervalo específico y luego desplazar ese intervalo a lo largo de la serie de datos para obtener una nueva serie de medias; lo que permite suavizar fluctuaciones en los datos así como resaltar tendencias.

Se conoce como media móvil ya que el valor se calcula constantemente a medida que pasa el tiempo; de esta forma, la media cambia cada vez que los valores presentan alguna modificación.


```{r echo=FALSE}
# Calcular el promedio móvil
data2$PromedioMovil <- rollmean(data2$Tasa, k = 3, fill = NA, align = "right")

# Visualizar la tasa original y el promedio móvil
ggplot(data2) +
  geom_line(aes(x = Fecha, y = Tasa), color = "blue", size = 1, group = 1) +  # Tasa original
  geom_line(aes(x = Fecha, y = PromedioMovil), color = "red", size = 1, group = 1) +  # Promedio móvil
  labs(title = "Tasa y Promedio Movil a lo largo del tiempo",
       x = "Fecha",
       y = "Tasa") +
  scale_x_date(date_labels = "%Y", date_breaks = "2 years") +  # Eje X de 2 en 2 años
  scale_y_continuous(breaks = seq(0, max(data2$Tasa, na.rm = TRUE), by = 2)) +  # Eje Y de 2 en 2
  theme_minimal(base_size = 15) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    axis.title = element_text(face = "bold"),
    panel.grid.major = element_line(color = "lightgrey"),
    panel.grid.minor = element_blank()
  )


```


## Rezago (operador backshift) y estacionalidad


El rezago es una herramienta estadística para el análisis de series temporales, que permite observar el valor de una variable en un momento anterior, facilitando la identificación de patrones y tendencias a lo largo del tiempo.

En cuanto a la estacionalidad, hace referencia a las variaciones periódicas y predecibles en los datos que ocurren en intervalos regulares.

Con la incorporación de ambas herramientas es posible modelar y prever comportamientos futuros de las series temporales.



```{r echo=FALSE, warning=FALSE}

# Aplicar rezago de 1 período 
data2$Tasa_lag1 <- dplyr::lag(data2$Tasa, n = 1)
head(data2)

```  
```{r echo=FALSE, warning=FALSE}

if (!is.ts(data2)) {
  data2 <- ts(data2)
}

# Verificar si hay valores NA o infinitos
if (any(is.na(data2)) || any(!is.finite(data2))) {
  data2 <- na.omit(data2)  # Eliminar valores NA
}

# Generar el gráfico de rezago
lag.plot(data2, lags = 3, do.lines = FALSE, main = "Grafico de Rezago")

```

La gráfica visualiza la correlación entre una variable y sus valores rezagados (delayed values). Este gráfico es útil para detectar patrones en series temporales y evaluar la autocorrelación en los datos.


* **Año vs Rezagos (lag 1, 2, 3)**

Las gráficas entre Año y sus diferentes rezagos muestran una relación lineal perfecta, lo que era de esperarse, ya que el valor de un año en un rezago anterior está directamente relacionado con los años consecutivos. Esto sugiere que el "Año" no aporta una variabilidad significativa en términos de cambios bruscos, es decir, la serie avanza sin saltos.


* **Mes vs Rezagos (lag 1, 2, 3)**

Las gráficas entre "Mes" y sus rezagos muestran una estructura cíclica, con puntos que siguen un patrón predecible. Esto tiene sentido, ya que los meses siguen un ciclo repetitivo de 12 unidades (de enero a diciembre).

Los meses correlacionan bien con sus rezagos inmediatos, pero a medida que aumenta el número de rezagos (lag 2, lag 3), el ciclo es más visible, lo que indica que la periodicidad estacional en los datos está bien representada.


* **Fecha vs Rezagos (lag 1, 2, 3)**

Similar al año, la relación entre "Fecha" y sus rezagos también muestra una estructura lineal. Esto era previsible, ya que las fechas están organizadas de manera continua. No se observan cambios abruptos o interrupciones que puedan señalar eventos singulares en la serie temporal.

* **Tasa vs Rezagos (lag 1, 2, 3)**

En las gráficas entre "Tasa" y sus rezagos, especialmente en el rezago 1 (lag 1), se aprecia una clara correlación positiva, lo que significa que la tasa en un mes está fuertemente relacionada con la tasa del mes anterior. Esta relación indica una persistencia en la tasa, es decir, no hay cambios abruptos entre periodos consecutivos.

A medida que el rezago aumenta (lag 2 y lag 3), la relación sigue siendo positiva pero disminuye levemente, lo cual es normal: los valores más distantes en el tiempo tienen menos influencia directa entre sí, aunque todavía se observa cierta correlación.

Esto sugiere que la serie de tasas no presenta grandes fluctuaciones a corto plazo y que los valores siguen un comportamiento más estable, lo que puede indicar una tendencia suave sin variaciones abruptas.


* **Promedio Movil vs Rezagos (lag 1, 2, 3)**

Similar a la "Tasa", las gráficas entre el "Promedio Móvil" y sus rezagos muestran una alta correlación, especialmente para lag 1. Esto sugiere que los promedios móviles no cambian drásticamente de un mes a otro, y los valores del promedio móvil están fuertemente ligados a los meses anteriores.
A medida que aumenta el rezago (lag 2, lag 3), la correlación disminuye ligeramente, lo que indica que los valores anteriores siguen teniendo una influencia pero con una menor magnitud.


* **Tasa Lag1 vs Rezagos**

Las gráficas entre "Tasa_lag1" y sus rezagos también muestran una alta correlación. Esto es de esperar, ya que los valores rezagados de una variable tienden a mostrar correlación fuerte con rezagos cercanos.

Los gráficos de rezago muestran una clara autocorrelación tanto en las variables de "Tas"a" como en el "Promedio Móvil", lo que sugiere que los valores actuales están muy influenciados por sus valores anteriores.

No se observan grandes fluctuaciones o cambios abruptos en la serie temporal, lo que implica que los datos de tasa y promedio móvil siguen una evolución suave a lo largo del tiempo.

La estructura cíclica en los meses sugiere que existe un patrón estacional predecible, lo cual es clave para definir modelos de predicción de series temporales con componentes estacionales, como modelos ARIMA/SARIMA o de descomposición estacional.



## Descomposición

Con la función stl(), se descompone la serie en tendencia, estacionalidad y componente residual.

```{r echo=FALSE}

data2 <- as.data.frame(data2)

# Convertir los datos a un objeto ts, usando frecuencia mensual
ts_data <- ts(data2[["Tasa"]], start = c(1999, 5), frequency = 12)

# Descomposición de la serie de tiempo
descomposicion <- stl(ts_data, s.window = "periodic")

# Graficar la descomposición
plot(descomposicion, main = "Descomposicion de la Serie de Tiempo")

```

Con el gráfico correcto de descomposición, evidenciamos:

* **Data (Serie Original):**

Se observa que las tasas de interés han pasado por varios ciclos a lo largo del tiempo, con un periodo inicial en el que las tasas eran más bajas (cerca de los años 2000). A partir de 2020, las tasas muestran un claro incremento, lo que puede reflejar una política monetaria más restrictiva o la respuesta del Banco de la República a factores como la inflación o la estabilidad macroeconómica.


* **Seasonal (Componente Estacional):**

El componente estacional revela un ciclo repetitivo bastante regular a lo largo de los años, con un patrón de estacionalidad que parece repetirse anualmente. La estacionalidad es un componente importante y muestra que ciertos meses o épocas del año presentan picos o caídas en la tasa de interés, posiblemente asociados con dinámicas de liquidez o factores económicos específicos (como ciclos agrícolas, comerciales o el impacto de eventos globales cíclicos).
Los picos más acentuados y las caídas rápidas indican que la estacionalidad tiene un impacto considerable, con cambios regulares de corto plazo que el Banco de la República puede usar para ajustar la política.


* **Trend (Componente de Tendencia):**

La tendencia de largo plazo muestra un ciclo de aumento hacia finales de los años 2000, seguido de una disminución suave en los años posteriores. Sin embargo, desde alrededor de 2020, se observa un repunte claro de las tasas.
Este aumento de las tasas a partir de 2020 puede estar relacionado con factores económicos recientes, como la pandemia de COVID-19, el aumento de la inflación global y las medidas que los bancos centrales, como el BanRep, tomaron para frenar la inflación y ajustar la política monetaria.
El gráfico sugiere que la tasa de intervención ha seguido un patrón cíclico a largo plazo, con fases alternas de crecimiento y caída.

* **Remainder (Componente Residual):**

El componente residual muestra las fluctuaciones que no pueden explicarse ni por la tendencia ni por la estacionalidad. Estas variaciones podrían deberse a eventos inesperados o choques externos que han afectado la política monetaria del país.
Se observan varios picos de volatilidad, como por ejemplo, alrededor del 2002, 2008-2009 (coincidiendo con la crisis financiera global), y más recientemente en torno a 2020, que podría estar relacionado con los efectos económicos de la pandemia.

La volatilidad hacia el final del gráfico es más alta, lo que podría indicar periodos de mayor incertidumbre económica o choques en los últimos años.


* **Tendencia de tasas crecientes:** El reciente aumento de las tasas de interés refleja probablemente un intento del Banco de la República de controlar la inflación y mantener la estabilidad macroeconómica en medio de un entorno de incertidumbre global.


* **Estacionalidad significativa:** El componente estacional muestra que la política monetaria sigue patrones regulares, lo cual puede estar vinculado con las necesidades cíclicas de liquidez en el mercado. Estos ciclos estacionales parecen estar bien definidos, lo que permite un ajuste más predecible de las tasas en el corto plazo.


* **Impacto de choques económicos:** El componente residual sugiere que eventos económicos inesperados han afectado la política monetaria, especialmente en periodos de crisis como 2008 y 2020. Estos choques pueden generar fluctuaciones a corto plazo que no son fácilmente predecibles.


## Estacionariedad

La prueba ADF indica si la serie tiene una raíz unitaria (es decir, si no es estacionaria).

```{r echo=FALSE, warning=FALSE}

# Prueba de Dickey-Fuller aumentada
adf_test <- tseries::adf.test(ts_data, alternative = "stationary")

# Mostrar resultado de la prueba ADF
print(adf_test)

```

El resultado del test muestra un valor de -1.15, con un p-valor de 0.9133. Dado que el p-valor es alto (mayor que un nivel de significancia común), sugiere que la serie temporal NO es estacionaria, por lo que podría requerir transformaciones adicionales, como la diferenciación, para hacerla estacionaria antes de modelarla.


## Diferenciación

Si una serie no es estacionaria, la diferenciación la ayuda a volverse estacionaria.

```{r echo=FALSE, warning=FALSE}
# Diferenciación de la serie
differenced_data <- diff(ts_data)

# Graficar la serie diferenciada
plot(differenced_data, main = "Serie Diferenciada", ylab = "Diferencia de Tasa", xlab = "Tiempo")

```

Al diferenciar la serie de tiempo, se observa en el gráfico  una serie fluctuante en torno a cero. Esto es una señal de que la diferenciación ayudó a eliminar la tendencia de la serie original, haciendo que los valores oscilen alrededor de un valor medio estable, lo que suele indicar un proceso estacionario.

## Volver a verificar la Estacionariedad tras la Diferenciación


Realizar nuevamente el test de Dickey-Fuller sobre la serie diferenciada permite confirmar la estacionariedad alcanzada:
 
 
```{r echo=FALSE, warning=FALSE}

# Prueba de Dickey-Fuller aumentada
adf_test2 <- tseries::adf.test(differenced_data, alternative = "stationary")

# Mostrar resultado de la prueba ADF
print(adf_test2)

```

El resultado del test de Dickey-Fuller aplicado a la serie diferenciada muestra un valor de estadístico de -3.0792 y un valor p de 0.1214. Aunque el estadístico es más bajo que en la serie inicial, el valor p sigue siendo mayor a 0.05, lo que significa que la serie diferenciada podría no ser completamente estacionaria, y podría sea necesario un segundo nivel de diferenciación o explorar otras transformaciones.


## Autocorrelación (ACF) y Parcial (PACF)

```{r echo=FALSE, warning=FALSE}

# ACF y PACF de la Serie Diferenciada
par(mfrow = c(1, 2))  
acf(differenced_data, main = "ACF de la Serie Diferenciada")
pacf(differenced_data, main = "PACF de la Serie Diferenciada")

```

* **ACF:** Las barras decrecientes sugieren una estructura de autocorrelación significativa, especialmente en los primeros rezagos, lo cual es típico en series con dependencia a corto plazo.

* **PACF:**  muestra una caída rápida después del primer rezago, lo cual sugiere que un modelo ARIMA de bajo orden podría ser adecuado para capturar la dinámica de la serie diferenciada.



```{r echo=FALSE, warning=FALSE}

# Descomposición de la serie de tiempo
descomposicion2 <- stl(differenced_data, s.window = "periodic")

# Graficar la descomposición
plot(descomposicion2, main = "Descomposicion TS DIFERENCIADA")

```

* **Data:** Se observa que a lo largo de la serie, existen varios picos, y la variabilidad parece aumentar en ciertos puntos. Esto indica que hay períodos con comportamiento anómalo o ruido.

* **Seasonal (Estacional):** La componente estacional muestra una clara periodicidad, con patrones que se repiten consistentemente en intervalos regulares. Esto indica que la serie tiene un comportamiento cíclico fuerte. Este componente estacional es útil para capturar patrones que se repiten cada cierto tiempo.

* **Trend (Tendencia):** La tendencia parece ser suave y muestra algunos cambios a lo largo del tiempo. Aunque la serie fue diferenciada, la tendencia residual indica que aún existen fluctuaciones de largo plazo, como un ligero aumento hacia el final de la serie. Esto sugiere que, aunque se haya eliminado una parte de la tendencia inicial mediante la diferenciación, todavía hay un componente de tendencia en los datos.

* **Remainder (Residuo):** Este componente representa la variabilidad no explicada por la estacionalidad ni la tendencia. Se observan varios picos y una dispersión más amplia en ciertos puntos de la serie. Esto sugiere que puede haber variaciones aleatorias o anomalías en ciertos periodos, especialmente al final, donde los residuos son más grandes. La presencia de valores residuales grandes refuerza la idea de que hay eventos impredecibles o ruido en la serie.

En resumen, la serie de tiempo diferenciada aún conserva una componente estacional significativa, y la tendencia residual es pequeña pero sigue presente. Los residuos presentan variaciones irregulares, especialmente al final, lo cual podría indicar la necesidad de modelar con más precisión estos picos o variabilidad.


# **Definición de Modelos**

## Modelo Arima

A continuación se probarán dos alternativas de modelo ARIMA, de acuerdo con los análisis realizados previamente, a fin de elegir aquel que se acomode mejor a la serie. 


**Alternativa 1 - c(1, 2, 3) (1, 1, 1):**

* Diferenciación no estacional (d): Dado que ya se aplicó una primera diferenciación en los datos y todavía no es completamente estacionaria, es posible considerar incrementar el parámetro de a 2. 

* Estacionalidad (seasonal): Como existe un patrón estacional claro en la serie, conviene incluir una diferenciación estacional (D = 1) y ajustar los parámetros P y Q para capturar las dependencias estacionales. Se probará el modelo con un seasonal = c(1, 1, 1) para incluir un término autoregresivo y uno de promedio móvil estacional.

* Componentes AR y MA (p y q): Dado que p y q determinan la cantidad de términos autoregresivos y de promedio móvil, conviene probar con valores más altos para q y ajustar p de acuerdo con la correlación en los residuos. Esto puede ayudar a mejorar la precisión del modelo y su capacidad predictiva.

```{r echo=FALSE, warning=FALSE}

# Ajuste del modelo ARIMA - ALTERNATIVA 1
modelo_arima <- Arima(differenced_data, order = c(1, 2, 3), seasonal = c(1, 1, 1))

# Resumen del modelo ajustado
summary(modelo_arima)

# Graficar los residuos para verificar la aleatoriedad
checkresiduals(modelo_arima)

# Pronosticar los próximos 72 períodos (6 años)
pronostico <- forecast(modelo_arima, h = 72)

# Graficar el pronóstico con el eje x adecuado
plot(pronostico, xlab = "Año", ylab = "Tasa", main = "Pronóstico de Tasa")

```

* **Parámetros no estacionales**

* p = 1 (un término autoregresivo),
* d = 2 (diferenciación de segundo orden para hacer estacionaria la serie en términos de tendencia),
* q = 3 (tres términos de promedio móvil).

* **parámetros estacionales*, donde:**

* P = 1 (un término autoregresivo estacional),
* D = 1 (diferenciación estacional),
* Q = 1 (un término de promedio móvil estacional),

* Los coeficientes y sus errores estándar sugieren que la mayoría de los términos son significativos (coeficientes altos en relación con sus errores estándar).


* El test de Ljung-Box evalúa si los residuos del modelo son independientes (no correlacionados). Con un p-valor de 0.1302, se sugiere que los residuos no tienen autocorrelación significativa. Esto indica que el modelo está capturando bien la estructura de la serie.

* Los resultados muestran que el modelo ARIMA(1,2,3)(1,1,1)[12] es probablemente adecuado para la serie.

* El p-valor del test de Ljung-Box mayor a 0.05 indica que el modelo ha eliminado la mayoría de la autocorrelación en los residuos, lo cual es positivo.


**Alternativa 1 - Análisis de Residuos**

```{r echo=FALSE, warning=FALSE}

# Normal QQ Plot de los residuos
residuos_diff <- residuals(modelo_arima)
par(mfrow = c(1, 1))  
qqnorm(residuos_diff, main = "Normal QQ Plot de Residuos 1")
qqline(residuos_diff, col = "red")

# Realizar el test de normalidad de Shapiro-Wilk
shapiro_test <- shapiro.test(residuos_diff)
print(shapiro_test)



```

Teniendo en cuenta la prueba de normalidad de Shapiro-Wilk, con un valor de 0.82525, siendo relativamente menor a 1, indica que los residuos no siguen una distribución normal.

En cuanto a p-value < 2.2e-16, al ser una cifra tan pequeña, se confirma que los residuos no son normales.

Por su parte, en la gráfica se evidencia un patrón en que los residuos tienen colas más gruesas que una distribución normal, lo cual reafirma la posible presencia de outliers o una distribución sesgada en los datos.


**Alternativa 1 - Análisis de volatilidad utilizando Garch**

```{r echo=FALSE, warning=FALSE}

# Análisis de volatilidad utilizando fGarch
garch_model <- garch(residuos_diff, order = c(1, 1))
summary(garch_model)



```

El modelo GARCH(1,1) parece ser adecuado para modelar la volatilidad de residuos_diff, ya que los coeficientes son significativos y el test de Box-Ljung no muestra autocorrelación en los residuos al cuadrado.

La alta significancia de los coeficientes ARCH (a1) y GARCH (b1) indica que tanto los residuos recientes como la volatilidad pasada influyen en la volatilidad actual.

La prueba de Jarque-Bera sugiere que los residuos no siguen una distribución normal, lo cual puede indicar la presencia de colas pesadas o asimetría, características típicas en series financieras.

Este modelo GARCH(1,1) es útil para capturar los cambios en la volatilidad de la serie residuos_diff. La ausencia de autocorrelación en los residuos al cuadrado también sugiere que el modelo ha capturado adecuadamente la heterocedasticidad de la serie.


**Auto Arima para identificar la Alternativa 2**

```{r echo=FALSE, warning=FALSE}
# Aplicar auto.arima para determinar los parámetros adecuados, incluyendo las diferenciaciones
modelo_auto <- auto.arima(differenced_data)
print(modelo_auto)
```

El modelo ARIMA(1,1,2) se seleccionó como el mejor ajuste para differenced_data basado en los valores del AIC, AICc, y BIC. La inclusión de términos AR y MA sugiere que hay dependencias de corto plazo en la serie, pero el ajuste no es perfecto, dado el tamaño de los errores estándar y el valor de sigma^2. Si el objetivo es la predicción, este modelo podría ser una buena base, aunque podrían explorarse otros modelos o ajustes adicionales para mejorar la precisión.




**Alternativa 2 - c(1, 1, 2) (1, 1, 0):**


```{r echo=FALSE, warning=FALSE}

# Ajuste del modelo ARIMA OPCIÓN 2
modelo_arima2 <- Arima(differenced_data, order = c(1, 1, 2), seasonal = c(1, 1, 0))
summary(modelo_arima2)
checkresiduals(modelo_arima2)

# Pronosticar los próximos 72 períodos (6 años)
pronostico2 <- forecast(modelo_arima2, h = 72)

# Graficar el pronóstico con el eje x adecuado
plot(pronostico2, xlab = "Año", ylab = "Tasa", main = "Pronóstico de Tasa")

```

*Autorregresivo:*
ar1 = 0.9274: Con un valor positivo, indica que existe una relación directa entre el valor actual de la serie y su valor anterior. Es decir, cuando el valor en el periodo t-1 aumenta, el valor en el periodo actual t tiende a aumentar también.

s.e. = 0.0393: Dado que el error estándar es bajo, indica que la estimación de este coeficiente es muy precisa. Esto sugiere que el modelo tiene una gran confianza en la relación directa entre el valor actual y el valor anterior de la serie temporal.

*Media Móvil:*
ma1 = -1.6115: El coeficiente negativo para ma1 indica que hubo un error negativo en la predicción en el periodo inmediatamente anterior (t-1). Es decir, el modelo predijo un valor más alto que el valor real en el periodo t-1, lo que resultó en un residuo negativo. Este residuo se ajusta en el modelo a través del término de media móvil.
s.e. = 0.0597: El error estándar para ma1 es relativamente bajo, lo que sugiere que esta estimación tiene alta precisión. En otras palabras, el ajuste de la predicción actual basado en el error de la predicción del periodo anterior es bastante confiable.

ma2 = 0.6144: Este coeficiente positivo para ma2 indica que el error positivo en la predicción de dos periodos atrás (t-2) tiene una relación directa con el valor actual de la serie; es decir, el modelo sobreestimó el valor real dos periodos atrás, lo que resultó en un residuo positivo que ahora ajusta la predicción para el periodo actual.
s.e. = 0.0583: Al igual que el error estándar de ma1, el error estándar de ma2 es bajo, lo que indica que esta estimación es también relativamente precisa.

*Autorregresivo estacional (SAR1):*
sar1 = -0.4744: El coeficiente negativo para sar1 sugiere que existe una relación inversa entre el valor actual de la serie y el valor de la serie con un rezago estacional de un periodo (es decir, el valor de la serie 12 periodos atrás). Si el valor de la serie en el periodo t-12 fue alto, el valor en el periodo actual t tenderá a ser más bajo. 
s.e. = 0.0620: El error estándar para sar1 es relativamente bajo, lo que indica que esta estimación también es bastante precisa y que el modelo tiene una buena confianza en el impacto de la componente estacional.

el modelo ARIMA(1,1,2)(1,1,0) tiene coeficientes precisos para los diferentes componentes (AR, MA y SAR), lo que sugiere que el modelo está bien ajustado y que las relaciones capturadas por los coeficientes son confiables.


**Alternativa 2 - Análisis de residuos**

```{r echo=FALSE, warning=FALSE}

# Análisis de Residuos Alternativa 2

# Normal QQ Plot de los residuos
residuos_diff2 <- residuals(modelo_arima2)
par(mfrow = c(1, 1))  
qqnorm(residuos_diff2, main = "Normal QQ Plot de Residuos 2")
qqline(residuos_diff2, col = "red")

# Realizar el test de normalidad de Shapiro-Wilk
shapiro_test2 <- shapiro.test(residuos_diff2)
print(shapiro_test)


```


Los resultados del test de Shapiro-Wilk indican que los residuos (residuos_diff) no siguen una distribución normal.

En cuanto a la gráfica también refleja la No normalidad. La desviación de los puntos en los extremos sugiere que los residuos tienen colas más pesadas de lo que se esperaría bajo una distribución normal. Este resultado es coherente con el Shapiro-Wilk test.


**Alternativa 2 - Análisis de volatilidad fGarch**

```{r echo=FALSE, warning=FALSE}

# Análisis de volatilidad utilizando fGarch
garch_model2 <- garch(residuos_diff2, order = c(1, 1))
summary(garch_model2)



```

* El modelo GARCH(1,1) parece capturar correctamente la volatilidad en los residuos, dado que no hay autocorrelación significativa en los residuos al cuadrado.

* Aunque los residuos no son normales (como muestra la prueba de Jarque-Bera), el modelo es aún válido, ya que los modelos GARCH no requieren normalidad en los residuos.

*El valor elevado de 𝑏1 (0.71542) indica una alta persistencia en la volatilidad, lo que es característico en este tipo de series de tiempo financieras.

En resumen, este modelo GARCH(1,1) parece adecuado para modelar la volatilidad de los residuos, aunque los residuos no sean normales.

**Comparación de modelos**


```{r echo=FALSE, warning=FALSE}

# Comparar criterios estadísticos
cat("Criterios estadísticos:\n")
cat("Modelo 1:\n")
cat("AIC:", AIC(modelo_arima), "\n")
cat("BIC:", BIC(modelo_arima), "\n\n")

cat("Modelo 2:\n")
cat("AIC:", AIC(modelo_arima2), "\n")
cat("BIC:", BIC(modelo_arima2), "\n\n")

# Análisis de residuos para ambos modelos
cat("Test de Ljung-Box para autocorrelación de residuos:\n")
cat("Modelo 1:\n")
Box.test(residuals(modelo_arima), lag = 20, type = "Ljung-Box")

cat("\nModelo 2:\n")
Box.test(residuals(modelo_arima2), lag = 20, type = "Ljung-Box")

# Test de normalidad para residuos
cat("\nTest de normalidad Shapiro-Wilk:\n")
cat("Modelo 1:\n")
shapiro.test(residuals(modelo_arima))

cat("\nModelo 2:\n")
shapiro.test(residuals(modelo_arima2))

# Calcular métricas de error (MAE, RMSE)
library(forecast)

cat("\nMétricas de error en datos de entrenamiento:\n")
cat("Modelo 1:\n")
accuracy(modelo_arima)

cat("\nModelo 2:\n")
accuracy(modelo_arima2)

# Graficar residuos
par(mfrow = c(2, 2))  # Dividir pantalla para gráficos

# Residuos Modelo 1
qqnorm(residuals(modelo_arima), main = "QQ Plot Residuos Modelo 1")
qqline(residuals(modelo_arima), col = "red")
plot(residuals(modelo_arima), main = "Residuos Modelo 1", ylab = "Residuos")
acf(residuals(modelo_arima), main = "ACF Residuos Modelo 1")

# Residuos Modelo 2
qqnorm(residuals(modelo_arima2), main = "QQ Plot Residuos Modelo 2")
qqline(residuals(modelo_arima2), col = "red")
plot(residuals(modelo_arima2), main = "Residuos Modelo 2", ylab = "Residuos")
acf(residuals(modelo_arima2), main = "ACF Residuos Modelo 2")

# Graficar y comparar pronósticos
par(mfrow = c(1, 1))
plot(pronostico2, xlab = "Año", ylab = "Tasa", main = "Comparación de Pronósticos")
lines(forecast(modelo_arima, h = 72)$mean, col = "blue", lty = 2)
legend("topleft", legend = c("Modelo 1", "Modelo 2"), col = c("blue", "red"), lty = c(2, 1))



```


A nivel general, 

* ALTERNATIVA  1:  muestra problemas de ajuste. Residuos no normales y autocorrelación presente. Aunque captura la tendencia, su precisión en los pronósticos parece más limitada.

* ALTERNATIVA  2:  ofrece un mejor ajuste. residuos más cercanos a la normalidad, sin autocorrelaciones significativas, y pronósticos con intervalos de confianza más ajustados. Esto lo hace preferible para tomar decisiones basadas en predicciones más consistentes.


En cuanto al análisis de pronótico:

La Alternativa 1 (línea azul punteada) y la Alternativa 2 (línea negra continua) ofrecen pronósticos similares en cuanto a tendencia, pero ela Alternativa 2 parece ser más conservadora, con menores amplitudes en sus intervalos de confianza.

Los intervalos de confianza de la  Alternativa 1  (zonas azules sombreadas) son más amplios, indicando mayor incertidumbre, mientras que en la 2 (zonas grises sombreadas) son más ajustados, lo que sugiere mayor precisión o menos variabilidad esperada, por tanto la 2 podría ser más confiable si se busca menor variabilidad en los pronósticos a largo plazo.


**En definitiva, ** la decisión es entonces optar por la Alternativa 2, ya que presenta un mejor comportamiento tanto en el ajuste a los datos como en la calidad de los pronósticos.


## Algoritmo de Holt Winter 

Puede manejar la estacionalidad en el conjunto de datos simplemente calculando el valor central y luego sumándolo o multiplicándolo por la pendiente y la estacionalidad. Solo tenemos que asegurarnos de ajustar el conjunto correcto de parámetros, y tenemos el mejor ajuste. 

Recuerde siempre verificar la eficiencia del modelo utilizando el valor MAPE (error porcentual absoluto medio) o el valor RMSE (error cuadrático medio), y la precisión puede depender del problema comercial y el conjunto de datos disponible para entrenar y probar el modelo.


Se observan la tendencia y los ciclos:

```{r echo=FALSE, warning=FALSE}
data2 = data

data2 <- data2 %>%
  mutate(Fecha = as.Date(Fecha), Anio = year(Fecha), Mes = month(Fecha, label = TRUE, abbr = TRUE)) %>%
  select(Anio, Mes, everything())

head(data2)


# Graficar solo líneas
plot(data2$Anio, data2$Tasa, type = "l", main = "Grafico con Regresion", xlab = "Anio", ylab = "Tasa")

# Ajustar el modelo de regresión lineal
modelo <- lm(Tasa ~ Anio, data = data2)

# Agregar la línea de regresión al gráfico
abline(modelo, col = "red")



```
```{r echo=FALSE, warning=FALSE}

# Graficar 
ggplot(data2, aes(x = Mes, y = Anio, fill = Tasa)) +
  geom_tile() +  # Usar geom_tile para crear un gráfico de calor
  scale_fill_gradient(low = "white", high = "blue") +  # Colores del llenado
  labs(title = "Tasa por Año y Mes", x = "Mes", y = "Año") +
  theme_minimal()



```

La gráfica sugiere una disminución en la tasa a lo largo del tiempo, con una estabilización en niveles bajos en los años recientes. Esto podría indicar una mejora en el fenómeno que se está midiendo con la "Tasa", aunque el contexto específico dependerá de qué represente exactamente esa variable en el análisis.

```{r echo=FALSE, warning=FALSE}
# serie_temporal <- ts(data2[["Tasa"]], start = c(min(data2$Anio), 1), frequency = 12)

# Extraer el ciclo de la serie temporal
cicle <- cycle(ts_data)

# Ver los ciclos
print(cicle)

```



```{r echo=FALSE, warning=FALSE}
# boxplot(data2~cycle(data2))

# Crear el boxplot por ciclo (mes) para observar la estacionalidad
boxplot(ts_data ~ cycle(ts_data), 
        xlab = "Mes", ylab = "Tasa", 
        main = "Boxplot de Tasa por Mes")

```

Aunque hay consistencia en la mediana y el rango intercuartílico de la tasa a lo largo de los meses, existen valores atípicos en la segunda mitad del año. Esto sugiere que, aunque la tasa general se mantiene estable, en algunos meses específicos ocurren eventos o circunstancias que resultan en tasas inusualmente altas.


```{r echo=FALSE, warning=FALSE}
# plot(log(airmiles), ylab='log(airmiles)', xlab='Anio', main='Logaritmo de la tasa')

plot(data2$Anio, log(data2$Tasa), type = "l", 
     ylab = "Log(Tasa)", xlab = "Año", 
     main = "Logaritmo de la Tasa a lo largo del Tiempo")
```

La gráfica sugiere que la tasa ha sido volátil a lo largo de los años, con periodos de estabilidad relativa intercalados con cambios bruscos.

La caída alrededor de 2020 y el rápido aumento posterior podrían reflejar eventos económicos o circunstancias externas que afectaron la tasa en ese periodo.

Esto fue evidenciado en el ANÁLISIS EXPLORATORIO.


## Modelo Holt-Winter

```{r echo=FALSE, warning=FALSE}
# modelo_HW = HoltWinters(log(airmiles), seasonal = "additive")
# plot(modelo_HW, main = 'Ajuste con Holt-Winters', xlab = 'Anio', ylab='log(airmiles)')
# Crear la serie temporal correctamente
ts_data <- ts(data2$Tasa, start = c(min(data2$Anio), min(data2$Mes)), frequency = 12)


# Aplicar el modelo Holt-Winters a la serie temporal
modelo_HW <- HoltWinters(ts_data)
print(modelo_HW)

# Graficar el ajuste del modelo a los datos históricos
plot(modelo_HW, main = "Ajuste del Modelo Holt-Winters")

# Pronóstico con Holt-Winters para los próximos 12 meses
pronostico_hw <- forecast(modelo_HW, h = 12)

# Graficar el pronóstico
plot(pronostico_hw, main = "Pronostico Holt-Winters para Tasa")


```



*Parámetro suavizado:*

Alpha (α = 0.9409767): Este es el parámetro de suavizado para el componente de nivel. Un valor cercano a 1, como el que está arrojando el resultado, indica que el modelo le da gran peso a los datos recientes para estimar el nivel actual de la serie. Esto significa que las observaciones recientes tienen un impacto significativo en la estimación del nivel.

Beta (β = 0.2941339): Este es el parámetro de suavizado para el componente de tendencia en el modelo de Holt-Winters. Un valor de β moderadamente bajo, como 0.2941339, sugiere que el modelo da un peso relativamente moderado a los cambios en la tendencia. Es decir, el modelo no es extremadamente sensible a las variaciones en la tendencia, pero sigue siendo capaz de capturarlas de manera significativa. Con este valor, el modelo ajusta la tendencia de forma más suave y gradual, lo que implica que no tiene una sobrereacción ante cambios recientes en la dirección de los datos.

Gamma (γ = 1): Este es el parámetro de suavizado para el componente estacional. Un valor de γ = 1 sugiere que el modelo da completo peso a los componentes estacionales previos en la serie.


```{r echo=FALSE, warning=FALSE}
plot(fitted(modelo_HW), main='Descomposicion con HW', xlab='Anio', ylab='log(airmiles)')
```


La tendencia ascendente en el nivel y la tendencia hacia el final del período (2025) sugiere que LA TASA está experimentando un crecimiento continuo.

La componente estacional muestra ciclos regulares bien definidos, lo que puede ser útil para predicciones estacionales precisas.

El modelo Holt-Winters parece ajustarse adecuadamente a la serie, capturando tanto las fluctuaciones estacionales como la tendencia de largo plazo.



**Se predice el método Holt Winters**

```{r echo=FALSE, warning=FALSE}
pred = predict(modelo_HW, 6, prediction.interval = TRUE)
pred
```

```{r echo=FALSE, warning=FALSE}
plot(modelo_HW, pred)
```



## Metodología Box Jenkins

Dentro de los pasos a seguir, tenemos:


1. Visualizar la serie.

2. Transformarla en estacionaria.

3. Graficar ACF - PACF, escoger los parámetros.

4. Construir el modelo ARIMA.

5. Hacer la predicción.


Los pasos anteriores ya habían sido previamente incorporados en instancias anteriores del análisis bajo otros modelo o metodologías, sin embargo, para fines de aplicar el método Box Jenkins realizaremos nuevamente el proceso y procedemos a calcular el punto de cambio de la media de la serie de tiempo inicial:

```{r echo=FALSE, warning=FALSE}

# Detectar punto de cambio en la media 
mval <- cpt.mean(ts_data, method = "AMOC")
cpts(mval)


```
Podemos identificar que sobre la serie original se encuentran 276 puntos de cambio de media.


```{r echo=FALSE, warning=FALSE}

# Graficar la serie con el punto de cambio resaltado
plot(mval, type = "l", cpt.col = "blue", xlab = "Índice de Tiempo", cpt.width = 4, 
     main = "Punto de Cambio en la Media")


```

Procedemos con el cálculo de la cantidad de diferenciaciones requeridas para hacer la serie estacionaria,en términos de media. Este análisis es crucial dentro del marco de la metodología Box-Jenkins, ya que el modelo ARIMA requiere que la serie sea estacionaria. Se observa que se requieren 2 diferenciaciones: 

```{r echo=FALSE, warning=FALSE}

ndiffs(ts_data)
differenced_data2 <- diff(differenced_data) #se genera la segunda diferenciación sobre differenced_data la cual ya era una diferenciación de la serie original ts_data.

```

Al correr nuevamente el código de punto de cambio de media (con **AMOC**)sobre la serie ya diferenciada 2 veces, el resultado es 0, indicando que no se ha detectado ningún cambio significativo en la media de dicha serie.

Esto se debe a que al diferenciar la serie, se eliminan componentes como tendencia y cambios en la media, lo que puede hacer que la serie parezca más "estable" en términos de media.


Incluso probando el método de cambio **PELT**, que es más flexible y busca múltiples puntos de cambio, el resultado sobre la serie diferenciado es 0, indicando que no se detectaron puntos de cambio significativos en la media de la serie de tiempo.

```{r echo=FALSE, warning=FALSE}

# Detectar punto de cambio en la media con AMOC
mval <- cpt.mean(differenced_data2, method = "AMOC")
cpts(mval)


mval_varios <- cpt.mean(differenced_data2, method = "PELT") # Detecta múltiples cambios
cpts(mval_varios)


```

A continuación usarmos la función **tso** para identificar valores atípicos en la serie de tiempo diferenciada (2 veces):


```{r echo=FALSE, warning=FALSE}


# Convertir differenced_data2 a un objeto de serie de tiempo con frecuencia 12 (pq la serie es mensual. 1 es si es diaria)

dat.ts <- ts(differenced_data2, frequency = 12)

# Detectar outliers en la serie diferenciada
data.ts.outliers <- tso(dat.ts, maxit.iloop = 50, maxit.oloop = 10)

# Mostrar los resultados de los outliers detectados
print(data.ts.outliers)
plot(data.ts.outliers)

```

* **Análisis de outliers:**

El análisis de outliers usa un modelo ARIMA para ajustar la serie original y detectar los puntos atípicos. En este caso, el modelo identificado es un ARIMA(1,0,1), lo que indica:

AR(1): Existe una correlación significativa con el valor inmediatamente anterior.
MA(1): Hay un efecto de ruido blanco ajustado con una media móvil de orden 1.
Diferenciación (0): La serie no fue diferenciada adicionalmente.

El modelo incluye los efectos de los outliers y ajusta la serie eliminando estas anomalías para capturar patrones más consistentes.


* **Eventos específicos en las fechas de los outliers:**

LS183 y LS190 (16to año, febrero y octubre) indican eventos que cambiaron el nivel de la serie. Esto puede ser consecuencia de eventos externos como políticas, cambios económicos o errores de registro.
Los cambios temporales (e.g., TC297) reflejan eventos transitorios.

Efectos significativos: LS305 (26:05) y TC297 (25:09) tienen impactos muy fuertes y requeriría investigarse a profundidad para identificar causas específicas.



* **Gráfico**

En la parte superior del gráfico se observan los puntos rojos que identifica dónde se encuentran los outliers. La línea azul muestra la serie original, mientras que la gris es la ajustada (sin los efectos de los outliers).

En la parte inferior del gráfico se muestra la magnitud de los impactos de cada outlier en la serie.



Se procede a aplicar el test de Dickey Fuller sobre la serie doblemente diferenciada:

```{r echo=FALSE, warning=FALSE}


adf.test(differenced_data2)

```



Se procede a aplicar análisis ACF y PACF sobre la serie doblemente diferenciada:

```{r echo=FALSE, warning=FALSE}
# Graficar ACF y PACF
par(mfrow = c(1, 2)) # Configurar para dos gráficos lado a lado
acf(differenced_data2, main = "Autocorrelación (ACF)")
pacf(differenced_data2, main = "Autocorrelación Parcial (PACF)")

```

* **ACF (Autocorrelación):**
Observamos un rezago significativo en el primer lag (1), que sobresale de los límites de confianza. Esto sugiere un componente MA(1) (promedio móvil de orden 1). A partir del segundo rezago, las autocorrelaciones caen rápidamente, lo que refuerza la hipótesis de un modelo MA de bajo orden.


* **PACF (Autocorrelación Parcial):**
El PACF muestra un rezago significativo en el primer lag (1), y los rezagos siguientes se reducen gradualmente. Esto indica que podría haber un componente AR(1) (autoregresivo de orden 1). Los rezagos subsiguientes no parecen ser significativos, lo que también respalda la hipótesis de un modelo AR de bajo orden.





```{r echo=FALSE, warning=FALSE}

modelo_autoarima <- auto.arima(differenced_data2, seasonal = FALSE)

# Resumen del modelo ajustado
summary(modelo_autoarima)

# Graficar el ajuste del modelo
plot(forecast(modelo_autoarima), main = "Pronóstico con auto.arima")

# Análisis de residuos
checkresiduals(modelo_autoarima)


```

* **Modelo Ajustado: ARIMA(1,0,2):**

El modelo seleccionado por auto.arima es un ARIMA(1,0,2) sin estacionalidad.


- p = 1: Un término autoregresivo.
- d = 0: No se aplicó diferenciación adicional porque la serie ya es estacionaria.
- q = 2: Dos términos de media móvil.
- seasonal = FALSE: No se incluyeron componentes estacionales, lo cual parece adecuado dado el comportamiento de la serie diferenciada.


* **Gráfica del Pronóstico:**

La línea azul representa el pronóstico central. Las bandas grises representan los intervalos de confianza del 80% y 95%. Dado que las bandas se ensanchan a medida que avanzan los períodos, esto refleja un aumento en la incertidumbre del pronóstico.


* **Análisis de Residuos (checkresiduals)**

- En este caso, los residuos no presentan una tendencia obvia ni autocorrelación fuerte, lo cual es un buen indicador.

- Dado que la mayoría de los puntos caen dentro de los límites de significancia (aproximadamente un nivel de confianza del 95%), no parece haber autocorrelación significativa, lo que indica que el modelo captura adecuadamente las dependencias temporales.

- El histograma muestra una distribución aproximadamente normal con una media cercana a cero, lo cual es importante para validar la aplicabilidad del modelo ARIMA.

* **Prueba de Ljung-Box (p-valor: 0.3103)**

Este valor es mayor que 0.05, lo que significa que no podemos rechazar la hipótesis nula, es decir que no hay evidencia de autocorrelación en los residuos, lo cual es un indicador de que el modelo ARIMA ajusta bien la serie.


* **Pronóstico**

```{r echo=FALSE, warning=FALSE}

# Generar pronósticos para los próximos 12 períodos
horizonte <- 6
pronostico <- forecast(modelo_autoarima, h = horizonte)

# Convertir el pronóstico en una tabla (data frame)
tabla_pronostico <- data.frame(
  Fecha = seq(from = as.Date("2024-11-01"), by = "month", length.out = horizonte),  # Fechas desde noviembre 2024
  Pronostico = as.numeric(pronostico$mean),           # Valores pronosticados
  Limite_Inferior_80 = as.numeric(pronostico$lower[, 1]),  # Límite inferior 80%
  Limite_Superior_80 = as.numeric(pronostico$upper[, 1]),  # Límite superior 80%
  Limite_Inferior_95 = as.numeric(pronostico$lower[, 2]),  # Límite inferior 95%
  Limite_Superior_95 = as.numeric(pronostico$upper[, 2])   # Límite superior 95%
)

# Mostrar la tabla
print(tabla_pronostico)



```

Es importante considerar que los datos del pronóstico están en escala pues la serie tiene 2 diferenciaciones, por tanto se procede a convertir a escala original:

```{r echo=FALSE, warning=FALSE}
# Últimos valores originales conocidos
ultimo_valor_original <- tail(ts_data, 1)  # Último valor de la serie original
ultimo_valor_diferenciado <- tail(differenced_data, 1)  # Último valor de la serie diferenciada una vez

# Revertir la segunda diferenciación
forecast_level1 <- cumsum(pronostico$mean) + rep(ultimo_valor_diferenciado, length(pronostico$mean))

# Revertir la primera diferenciación
forecast_final <- cumsum(forecast_level1) + rep(ultimo_valor_original, length(forecast_level1))

# Crear tabla con los valores des-diferenciados
tabla_pronostico_ARIMA_original <- data.frame(
  Fecha = seq(from = as.Date("2024-11-01"), by = "month", length.out = horizonte),
  Pronostico = forecast_final
)

# Mostrar resultados
print(tabla_pronostico_ARIMA_original)



```


## Modelo Prophet

Para trabajar este modelo, partimos de la data original pues el modelo Prophet no requiere que la serie  sea diferenciada previamente porque internamente utiliza una combinación de métodos que manejan las tendencias y estacionalidades de manera explícita, evitando la necesidad de transformar la serie para hacerla estacionaria, como se hace en modelos como ARIMA. 


```{r echo=FALSE, warning=FALSE}

data3 <- data

data3 <- data3 %>%
  arrange(Fecha)  # Ordena las fechas


data3$Fecha <- as.Date(data3$Fecha, format = "%Y-%m-%d")




# Renombrar las columnas para prophet
prophet_data <- data.frame(
  ds = data3$Fecha,  # Fecha en la columna 'ds'
  y = data3$Tasa  # La columna 'Tasa' en 'y'
)

# Crear y ajustar el modelo
model <- prophet(prophet_data)

# Crear un DataFrame para 6 meses hacia adelante sin incluir las fechas históricas
future <- make_future_dataframe(model, periods = 6, freq = "month", include_history = FALSE)

# Generar las predicciones
forecast <- predict(model, future)

# Mostrar las primeras filas de las predicciones
head(forecast[, c("ds", "yhat", "yhat_lower", "yhat_upper")])

```


* Valores centrales **(yhat)** Son las predicciones para la variable Tasa, desde noviembre de 2024 hasta abril de 2025. Por ejemplo, para noviembre de 2024, se predice que la tasa será de aproximadamente 10.03%; los valores aumentan ligeramente hacia principios de 2025, lo que podría indicar una tendencia al alza.


* Los intervalos de confianza **(yhat_lower y yhat_upper)**, indican la incertidumbre asociada a cada predicción.
Por ejemplo, para noviembre de 2024, el modelo predice que la tasa podrá oscilar entre 6.71 y 13.31 con un nivel de confianza alto. Dado que los intervalos son amplios, sugiere que hay incertidumbre considerable en las proyecciones a futuro (esto puede originarse por una alta volatilidad en los datos históricos).


* Se evidencia también que las predicciones para fechas más cercanas, como noviembre y diciembre de 2024, tienen menor incertidumbre (IC más estrecho) en comparación con fechas más lejanas, como marzo y abril de 2025. Esto es esperado porque la precisión del modelo suele disminuir con el tiempo.



```{r echo=FALSE, warning=FALSE}


# Graficar predicciones
plot(model, forecast)
```

* Los puntos negros representan los datos históricos de la serie temporal. La gráfica muestra una tendencia con picos y valles notables, lo que podría indicar una serie con alta variabilidad (esto sustenta lo mencionado anteriormente) y posiblemente algún patrón cíclico o estacionalidad.

* En la parte derecha de la gráfica, se observa una línea azul que representa la predicción del modelo para el futuro cercano.

* Las áreas sombreadas en azul alrededor de las predicciones representan los intervalos de confianza del modelo, que indican el rango dentro del cual es más probable que se encuentren los valores reales. Estos intervalos son más amplios en la predicción debido a la incertidumbre inherente a los modelos predictivos.

* El modelo parece ajustarse bien a los datos históricos y proporciona predicciones con una incertidumbre razonable. Se podría inferir que los valores futuros tienen una probabilidad de mantenerse en el rango predicho, sujeto a factores externos.



```{r echo=FALSE, warning=FALSE}


# Graficar los componentes del modelo
prophet_plot_components(model, forecast)


```

Esta gráfica muestra los componentes del modelo generado con Prophet, descomponiendo la serie temporal en tendencia (trend) y estacionalidad anual (yearly).

* **Tendencia:** El gráfico muestra cómo evoluciona la tendencia general a lo largo del tiempo (noviembre a abril).
Se observa un incremento gradual en los valores a medida que avanza el tiempo. Esto indica una tendencia positiva sostenida en la serie temporal, lo que puede implicar crecimiento o recuperación en el fenómeno estudiado.

La banda sombreada alrededor de la línea representa el intervalo de confianza del modelo, indicando la incertidumbre asociada a las predicciones. En este caso, la incertidumbre es baja, ya que la banda es estrecha.


* **Estacionalidad anual:** Este gráfico captura patrones repetitivos que ocurren dentro de un año.
Se observan picos y valles a lo largo del año, lo que sugiere que el fenómeno presenta una fuerte estacionalidad.

Los picos indican que los valores tienden a ser más altos en ese mes, mientras que los valles muestran que los valores disminuyen en ese período.Entre julio y octubre, se observa una fluctuación más moderada.

* En resumen, el modelo identifica una combinación de una tendencia creciente y un patrón de estacionalidad anual, lo que es clave para entender y predecir el comportamiento del fenómeno a futuro.



## Redes Neuronales


**Preparación de los Datos**

A continuación vamos a normalizar la columna Tasa y dividir los datos en conjuntos de entrenamiento y prueba. 

```{r echo=FALSE, warning=FALSE}

data2 = data

# Normalización de la columna 'Tasa' entre 0 y 1
normalize <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}

data2$Tasa_norm <- normalize(data2$Tasa)

# Dividir los datos en entrenamiento (75%) y prueba (25%)

set.seed(123)
train_indices <- sample(1:nrow(data2), 0.75 * nrow(data2))
train_data <- data2[train_indices, ]
test_data <- data2[-train_indices, ]

```



```{r echo=FALSE, warning=FALSE}
# Calcular el origen de las fechas
fecha_max <- as.Date("2024-10-01")  # Última fecha conocida
dias_max <- 19905                  # Valor numérico de la última fecha

# Calcular el origen restando los días
fecha_origen <- fecha_max - dias_max

# Convertir la columna Fecha en formato Date
test_data$Fecha <- as.Date(test_data$Fecha, origin = as.character(fecha_origen))



```


**Modelo ELMAN**

A continuación se entrena una red neuronal de tipo Elman, con el fin de predecir la Tasa, esto a través del uso de un conjunto de datosd e entrenamiento.

Para esto, se realiza el siguiente proceso:

* Se prepara la entrada y salida del modelo utilizando los datos normalizados.
* Se crea y entrena la red Elma con 10 neuronas en la capa oculta, una tasa de aprendizaje del 5% y un máximo de 1.000 iteraciones.

Los valores predichos por el modelo se encuentran en un rango cercano entre 11.79 y 13.31. Esto sugiere que el modelo ha aprendido un patrón en los datos de entrada y salida, y las predicciones son coherentes dentro de un rango esperado.

```{r echo=FALSE, warning=FALSE}

# Preparar los datos de entrada y salida
train_input <- as.matrix(train_data$Tasa_norm)
train_output <- as.matrix(train_data$Tasa_norm)

# Crear y entrenar la red Elman
set.seed(123)
elman_model <- elman(
  x = train_input, 
  y = train_output, 
  size = c(10),  # 10 neuronas en la capa oculta
  learnFuncParams = c(0.05),  # Tasa de aprendizaje
  maxit = 1000,  # Máximo número de iteraciones
  linOut = TRUE  # Salida lineal para problemas de regresión
)

# Predicciones en el conjunto de prueba
test_input <- as.matrix(test_data$Tasa_norm)
predictions_elman <- predict(elman_model, test_input)

# Desnormalizar las predicciones
denormalize <- function(x, original) {
  x * (max(original) - min(original)) + min(original)
}


predictions_elman_desnorm <- denormalize(predictions_elman, data2$Tasa)


test_data$Predicciones_Elman <- predictions_elman_desnorm


# Extraer solo las columnas "Fecha" y "Predicciones_Elman"
predicciones_con_fecha <- test_data[, c("Fecha", "Predicciones_Elman")]

# Ver el resultado
head(predicciones_con_fecha)


```

El código a continuación, evalúa el rendimiento del modelo Elman al calcular la correlación entre las predicciones realizadas por el modelo y los valores reales del conjunto de prueba. Utilizando la función cor(), se obtiene el coeficiente de correlación de Pearson, que mide la relación lineal entre las predicciones y los valores observados. 

Al obtener un resultado de 0.9999664, y al ser un valor muy cercano a 1, se entiende como una alta correlación entre las predicciones del modelo y los datos reales. Por lo tanto, el modelo de redes neuronales de Elman está prediciendo adecuadamente los valores en el conjunto de prueba.



```{r echo=FALSE, warning=FALSE}

# Evaluación
actuals <- test_data$Tasa
print(cor(predictions_elman, actuals))  # Correlación entre predicciones y valores reales

```



**Predicciones futuras ELMAN:**

Con el siuguiente código se realiza una predicción de valores futuros utilizando el modelo previo de Elman. Se crea una secuencia de fechas de seis meses a partir de noviembre de 2024. Luego, se toma el último valor de las predicciones no desnormalizadas para usarlo como entrada inicial para generar las predicciones futuras. Posteriormente el código ejecuta un ciclo que para cada fecha futura predice el siguiente valor con el modelo de Elman, actualizando la entrada para la siguiente predicción con el valor pronosticado previamente. 

Teniendo en cuenta el resultado de Elman, se tendría un crecimiento gradual en la tasa,para los siguientes seis meses (de noviembre 2024 a abril 2025); pasando de 23.90% a 24.13% respectivamente.

```{r echo=FALSE, warning=FALSE}

# Crear una secuencia de fechas futuras. 6 meses a partir de noviembre de 2024
fechas_futuras <- seq(as.Date("2024-11-01"), by = "month", length.out = 6)

# Obtener el último valor de las predicciones no desnormalizadas
ultimo_valor_no_desnormalizado <- predictions_elman[length(predictions_elman)]

# Crear una matriz con la última predicción como entrada inicial
input_futuro <- as.matrix(c(ultimo_valor_no_desnormalizado))

# Inicializar un vector para almacenar las predicciones futuras
predicciones_futuras <- numeric(length(fechas_futuras))

# Generar predicciones futuras
for (i in 1:length(fechas_futuras)) {
  # Predecir el siguiente valor usando el modelo Elman
  predicciones_futuras[i] <- predict(elman_model, input_futuro)
  
  # Actualizar la entrada para la siguiente predicción
  input_futuro <- as.matrix(predicciones_futuras[i])
  
  # Imprimir cada predicción futura para revisar el proceso
  cat("Predicción para", fechas_futuras[i], ": ", predicciones_futuras[i], "\n")
}

# Desnormalizar las predicciones futuras
predicciones_futuras_desnormalizadas <- denormalize(
  x = predicciones_futuras,
  original = data2$Tasa
)

# Crear el dataframe con las predicciones futuras desnormalizadas
data_futuro_desnormalizado <- data.frame(
  Fecha = fechas_futuras,
  Predicciones_Elman_Desnorm = predicciones_futuras_desnormalizadas
)

# Mostrar las predicciones futuras desnormalizadas
print(data_futuro_desnormalizado)



```


A continuación se muestra la gráfica, en donde se puede visualizar la predicción de la tasa para los seis meses.

```{r echo=FALSE, warning=FALSE}


ggplot(data_futuro_desnormalizado, aes(x = Fecha, y = Predicciones_Elman_Desnorm)) +
  geom_line(color = "blue") +
  geom_point(color = "red") +
  labs(title = "Pronóstico de la Tasa con Red Elman",
       x = "Fecha",
       y = "Predicción de Tasa") +
  theme_minimal()

```

Integrar los datos con las predicciones pasadas


```{r echo=FALSE, warning=FALSE}

# Verificar los nombres de las columnas en ambos dataframes
colnames(data_futuro_desnormalizado)
colnames(predicciones_con_fecha)

# Renombrar las columnas para que coincidan
colnames(data_futuro_desnormalizado)[colnames(data_futuro_desnormalizado) == "Predicciones_Elman_Desnorm"] <- "Predicciones_Elman"
colnames(predicciones_con_fecha)[colnames(predicciones_con_fecha) == "Predicciones_Elman"] <- "Predicciones_Elman"

# Combinar los datos
data_completo <- rbind(data_futuro_desnormalizado, predicciones_con_fecha)

# Convertir la columna Fecha a tipo Date y ordenar los datos por fecha
data_completo$Fecha <- as.Date(data_completo$Fecha)
data_completo <- data_completo[order(data_completo$Fecha, decreasing = TRUE), ]

# Imprimir los registros más recientes
head(data_completo, 12)



```

A continuación se muestra el gráfico con los datos pasados así como con las predicciones:

```{r echo=FALSE, warning=FALSE}

ggplot(data_completo, aes(x = Fecha, y = Predicciones_Elman)) +
  geom_line(color = "blue") +
  geom_point(color = "red") +
  labs(title = "Predicciones pasadas y pronóstico futuro",
       x = "Fecha",
       y = "Predicción de Tasa") +
  theme_minimal()


```




**Modelo JORDAN**

El modelo de red neuronal de tipo Jordan, permite captar patrones temporales y secuenciales. En este caso, la red se entrena utilizando los datos de entrada (train_input) y las salidas deseadas (train_output), con 5 neuronas en la capa oculta y una tasa de aprendizaje de 10%. El entrenamiento se realiza en un máximo de 500 iteraciones. Después de entrenar el modelo, se realizan predicciones sobre el conjunto de prueba (test_input), que luego se desnormalizan utilizando los valores de la tasa de la serie temporal original.

El resultado de  0.9994467, muestra que el modelo es muy eficiente para predecir las tasas.


```{r echo=FALSE, warning=FALSE}

# Crear y entrenar la red Jordan
set.seed(123)
jordan_model <- jordan(
  x = train_input, 
  y = train_output, 
  size = c(5),  # 5 neuronas en la capa oculta
  learnFuncParams = c(0.1),  # Tasa de aprendizaje
  maxit = 500,  # Máximo número de iteraciones
  linOut = TRUE  # Salida lineal para problemas de regresión
)

# Predicciones en el conjunto de prueba
predictions_jordan <- predict(jordan_model, test_input)

# Desnormalizar las predicciones
predictions_jordan_desnorm <- denormalize(predictions_jordan, data2$Tasa)

# Crear un DataFrame con las predicciones y las fechas correspondientes
predicciones_con_fechas_jordan <- data.frame(
  Fecha = test_data$Fecha,               # Fechas correspondientes
  Predicción_Jordan = predictions_jordan_desnorm  # Predicciones de Jordan
)

# Ver las primeras filas para verificar
head(predicciones_con_fechas_jordan)



```

```{r echo=FALSE, warning=FALSE}

# Evaluación
print(cor(predictions_jordan_desnorm, actuals))  # Correlación entre predicciones y valores reales


```


**Predicciones futuras JORDAN:**

Con el modelo Jordan, se muestra para las predicciones futuras una disminución de la tasa, pasando de 21.84% a 17.16% en seis meses.

```{r echo=FALSE, warning=FALSE}
# Crear una secuencia de fechas futuras (6 meses desde noviembre de 2024)
fechas_futuras <- seq(as.Date("2024-11-01"), by = "month", length.out = 6)

# Normalizar el último valor de las predicciones (usando min y max de la columna Tasa de data2)
normalize_single_value <- function(x, min_value, max_value) {
  (x - min_value) / (max_value - min_value)
}

# Obtener el último valor desnormalizado de las predicciones de Jordan
ultimo_valor_desnormalizado <- predictions_jordan_desnorm[length(predictions_jordan_desnorm)]

# Normalizar el último valor usando el min y max de la columna Tasa de data2
ultimo_valor_normalizado <- normalize_single_value(ultimo_valor_desnormalizado, min(data2$Tasa), max(data2$Tasa))

# Crear una matriz con la última predicción normalizada como entrada inicial
input_futuro <- as.matrix(c(ultimo_valor_normalizado))

# Inicializar un vector para almacenar las predicciones futuras
predicciones_futuras_jordan <- numeric(length(fechas_futuras))

# Generar las predicciones futuras
for (i in 1:length(fechas_futuras)) {
  # Predecir el siguiente valor usando el modelo Jordan
  predicciones_futuras_jordan[i] <- predict(jordan_model, input_futuro)
  
  # Actualizar la entrada para el siguiente paso
  input_futuro <- as.matrix(predicciones_futuras_jordan[i])
  
  # Imprimir cada predicción futura
  cat("Predicción para", fechas_futuras[i], ": ", predicciones_futuras_jordan[i], "\n")
}

# Desnormalizar las predicciones futuras
predicciones_futuras_jordan_desnorm <- denormalize(
  x = predicciones_futuras_jordan,
  original = data2$Tasa
)

# Crear el dataframe con las predicciones futuras desnormalizadas
data_futuro_jordan <- data.frame(
  Fecha = fechas_futuras,
  Predicción_Jordan = predicciones_futuras_jordan_desnorm
)

# Ver las predicciones futuras
print(data_futuro_jordan)


```

```{r echo=FALSE, warning=FALSE}
# Gráfico con las predicciones futuras para el modelo Jordan
ggplot(data_futuro_jordan, aes(x = Fecha, y = Predicción_Jordan)) +
  geom_line(color = "blue") +
  geom_point(color = "red") +
  labs(title = "Pronóstico de la Tasa con Red Jordan",
       x = "Fecha",
       y = "Predicción de Tasa") +
  theme_minimal()



```

Similar al modelo Elman, también se evidencia un salto en la tasa, correspondiente a los meses de octubre a noviembre del 2024.



```{r echo=FALSE, warning=FALSE}



# Verificar los nombres de las columnas en ambos dataframes
colnames(data_futuro_jordan) # Este es el dataframe con las predicciones futuras de Jordan
colnames(predicciones_con_fechas_jordan) # Este es el dataframe con las predicciones pasadas de Jordan

# Renombrar las columnas para que coincidan
colnames(data_futuro_jordan)[colnames(data_futuro_jordan) == "Predicción_Jordan"] <- "Predicciones_Jordan"
colnames(predicciones_con_fechas_jordan)[colnames(predicciones_con_fechas_jordan) == "Predicción_Jordan"] <- "Predicciones_Jordan"

# Combinar los datos (predicciones futuras y pasadas de Jordan)
data_completo_jordan <- rbind(data_futuro_jordan, predicciones_con_fechas_jordan)

# Convertir la columna Fecha a tipo Date y ordenar los datos por fecha
data_completo_jordan$Fecha <- as.Date(data_completo_jordan$Fecha)
data_completo_jordan <- data_completo_jordan[order(data_completo_jordan$Fecha, decreasing = TRUE), ]

# Imprimir los registros más recientes
head(data_completo_jordan, 12)





```


```{r echo=FALSE, warning=FALSE}

# Graficar las predicciones pasadas y pronóstico futuro para el modelo Jordan
ggplot(data_completo_jordan, aes(x = Fecha, y = Predicciones_Jordan)) +
  geom_line(color = "blue") +               # Línea para las predicciones
  geom_point(color = "red") +               # Puntos para las predicciones
  labs(title = "Predicciones pasadas y pronóstico futuro con Red Jordan",
       x = "Fecha",
       y = "Predicción de Tasa") +           # Etiquetas de los ejes
  theme_minimal()                           # Estilo minimalista



```


**Comparación de Resultados de ambas Redes**

El resultado a continuacio´n muestra una comparación entre las predicciones generadas por los modelos anteriormente realizados: Elman y Jordan. Las predicciones de ambos modelos se acercan a los valores reales, aunque con algunas diferencias notables. Por ejemplo, en las fechas más recientes como octubre y septiembre de 2024, ambas predicciones (Elman y Jordan) están muy cerca de los valores reales (11.75), pero el modelo Elman presenta un ligero mejor ajuste con valores de 11.790453 y 11.789933, respectivamente, en comparación con los valores del modelo Jordan, que son 11.815554 y 11.799799. En fechas anteriores, como en diciembre de 2023, los dos modelos también muestran predicciones bastante cercanas entre sí, con ligeras variaciones, siendo el modelo Elman más cercano al valor real (13.300097 frente a 13.274618). A lo largo de las fechas más antiguas, ambas predicciones siguen una tendencia similar, pero con pequeñas diferencias, lo que sugiere que el modelo Elman tiende a ajustarse de manera más precisa a los datos reales en comparación con el modelo Jordan, aunque ambos modelos muestran un rendimiento aceptable en la mayoría de los casos.


```{r echo=FALSE, warning=FALSE}

# Crear un DataFrame con los resultados
prediction_table <- data.frame(
  Fecha = test_data$Fecha,                    # Fechas correspondientes
  Actual = actuals,                           # Valores reales
  Predicción_Elman = predicciones_con_fecha$Predicciones_Elman,  # Predicciones de Elman 
  Predicción_Jordan = predicciones_con_fechas_jordan$Predicciones_Jordan     # Predicciones de Jordan
)

# Mostrar las primeras filas de la tabla
head(prediction_table)

# Imprimir toda la tabla 
print(prediction_table)



```

A continuación, se visualiza una gráfica con las predicciones comparativas entre los modelos Elman y Jordan.

```{r echo=FALSE, warning=FALSE}
library(ggplot2)

# Crear un DataFrame para comparar
results <- data.frame(
  Fecha = test_data$Fecha,               # Fechas correspondientes
  Actual = actuals,                      # Valores reales
  Elman = predicciones_con_fecha$Predicciones_Elman,             # Predicciones de Elman
  Jordan = predicciones_con_fechas_jordan$Predicciones_Jordan            # Predicciones de Jordan
)

# Verificar que no hay NA o diferencias en las longitudes
summary(results)

# Gráfico
ggplot(results, aes(x = Fecha)) +
  geom_line(aes(y = Actual, color = "Actual")) +
  geom_line(aes(y = Elman, color = "Elman")) +
  geom_line(aes(y = Jordan, color = "Jordan")) +
  labs(title = "Comparación de predicciones", 
       y = "Tasa", 
       color = "Leyenda") +
  theme_minimal()





```
# **Conclusiones Finales**

La Tasa de Intervención del Banco de la República constituye un indicador clave de la política monetaria colombiana, siendo influenciada por decisiones estratégicas para estabilizar la inflación y el valor de la moneda. Su comportamiento combina tendencias, estacionalidad, y fluctuaciones impredecibles asociadas con eventos externos o decisiones abruptas del Banco Central.

Con el propósito de entender mejor el comportamiento histórico de la serie y de probar la capacidad predictiva de diversos modelos, se llevó a cabo un análisis exploratorio y se elaboraron los siguientes modelos proyectivos:

* ARIMA (AutoRegressive Integrated Moving Average)
* Holt-Winters (Descomposición de estacionalidad y suavizamiento exponencial)
* Prophet (Modelo ajustable de tendencia y estacionalidad desarrollado por Meta)
* Redes Neuronales (Elman y Jordan)

El análisis se basó en datos mensuales desde 1999 hasta 2024, abarcando un periodo extenso que incluye diversas dinámicas económicas y cambios estructurales en el país.



**Comparación Global de Modelos**


```{r echo=FALSE, message=FALSE, warning=FALSE}

pronostico_hw = pred
pronostico_arima = tabla_pronostico_ARIMA_original
pronostico_pp = forecast
pronostico_elm = data_futuro_desnormalizado
pronostico_jrd = data_futuro_jordan



if (is.matrix(pronostico_hw)) {
  pronostico_hw <- as.data.frame(pronostico_hw)
}

fechas_hw <- seq(from = as.Date("2024-11-01"), by = "month", length.out = nrow(pronostico_hw))

pronostico_hw <- data.frame(
  Fecha = fechas_hw,          # Columna de fechas
  Pronostico_hw = pronostico_hw[, "fit"]  # Valores ajustados desde la columna "fit"
)


  
# Verificar si pronostico_pp es un dataframe
if (is.data.frame(pronostico_pp)) {
  # Extraer las fechas
  fechas_pp <- pronostico_pp$ds
  
  # Crear el data.frame con las columnas necesarias
  pronostico_pp <- data.frame(
    Fecha = fechas_pp,               # Columna de fechas
    Pronostico_pp = pronostico_pp$trend # Columna de pronóstico, que es el valor de la tendencia
  )
  
}

# print(pronostico_hw)
# print(pronostico_arima)
# print(pronostico_pp)
# print(pronostico_elm)
# print(pronostico_jrd)


pronostico_pp$Fecha <- as.Date(pronostico_pp$Fecha)
pronostico_hw$Fecha <- as.Date(pronostico_hw$Fecha)
pronostico_arima$Fecha <- as.Date(pronostico_arima$Fecha)
pronostico_elm$Fecha <- as.Date(pronostico_elm$Fecha)
pronostico_jrd$Fecha <- as.Date(pronostico_jrd$Fecha)

# Crear la tabla comparativa uniendo todas las predicciones
tabla_comparativa <- pronostico_hw %>%
  rename(Pronostico_hw = Pronostico_hw) %>%  # Ya está correcto, no es necesario cambiar
  full_join(pronostico_arima %>% rename(Pronostico_arima = Pronostico), by = "Fecha") %>%
  full_join(pronostico_pp %>% rename(Pronostico_pp = Pronostico_pp), by = "Fecha") %>%
  full_join(pronostico_elm %>% rename(Predicciones_Elman = Predicciones_Elman), by = "Fecha") %>%
  full_join(pronostico_jrd %>% rename(Predicciones_Jordan = Predicciones_Jordan), by = "Fecha")

# Crear la columna 'Valor real' con los valores para los periodos de noviembre y diciembre de 2024
valor_real <- rep(NA, nrow(tabla_comparativa))  # Inicializar la columna con NA

# Asignar el valor 9.75 a las filas correspondientes a noviembre y diciembre de 2024
valor_real[tabla_comparativa$Fecha == "2024-11-01"] <- 9.75
valor_real[tabla_comparativa$Fecha == "2024-12-01"] <- 9.75

# Añadir la columna 'Valor real' al inicio de la tabla comparativa
tabla_comparativa <- tabla_comparativa %>%
  mutate(Valor_real = valor_real) %>%
  select(Fecha, Valor_real, everything())  # Reorganizar para poner 'Valor_real' al inicio



print(tabla_comparativa)



```


```{r echo=FALSE, warning=FALSE}

library(knitr)

# Mostrar la tabla con kable
kable(tabla_comparativa, format = "markdown", caption = "Tabla Comparativa de Pronósticos")


```


La variabilidad entre los modelos es notable. Mientras que Holt-Winters, ARIMA y Prophet sugieren un crecimiento, Elman y Jordan sugieren una disminución, lo que podría reflejar diferencias en los métodos y tipos de datos que cada uno de estos modelos utiliza. Esto sugiere que hay divergencias significativas en la naturaleza de los datos que estos modelos están capturando.

En cuanto al ajuste, dado que se espera que la tasa se estabilice o disminuya (y esto se puede evidenciar en el comportamiento de noviembre y diciembre), los modelos Elman y Jordan podrían estar capturando la tendencia de manera más precisa.

Se evidencia también que Prophet es el modelo más preciso en cuanto a la cercanía de sus predicciones respecto a los valores reales, esto sugiere que tiene un buen ajuste con los patrones de los datos actuales. Esto no significa necesariamente que siempre será el más preciso, pero en este caso parece estar capturando mejor las dinámicas subyacentes. Sin embargo, para asegurarse de su validez futura, es recomendable continuar evaluando su desempeño y ajustarlo si es necesario.


En definitiva, el análisis realizado sobre la tasa de intervención del Banco de la República de Colombia ha demostrado que los diferentes modelos aplicados (ARIMA, Holt-Winters, Prophet y Redes Neuronales) tienen capacidades complementarias para capturar las dinámicas de esta serie temporal, cada uno con fortalezas específicas.

De manera general, las Redes Neuronales, en especial la Red Elman, destacan por su habilidad para modelar relaciones complejas y no lineales en la serie. No obstante, modelos como Prophet son opciones igualmente válidas debido a su flexibilidad para incorporar eventos atípicos y su facilidad de configuración. 

Tanto para los modelos estadísticos como para las redes neuronales, es crucial realizar una búsqueda sistemática de hiperparámetros para garantizar un ajuste óptimo, por ejemplo, en las Redes Neuronales, ajustar el número de neuronas en la capa oculta, variar la tasa de aprendizaje y los coeficientes de regularización. En el caso de Prophet, optimizar la configuración de estacionalidad y ajustar la sensibilidad a eventos atípicos o discontinuidades en la serie.

También podría considerarse mejorar la capacidad predictiva mediante la incorporación de factores económicos adicionales como:

* Inflación (IPC).
* Tasas de cambio (peso frente al dólar).
* Precios del petróleo, dado su impacto en la economía colombiana.
* Indicadores internacionales como tasas de interés de la Reserva Federal (FED).


También es pertinente realizar validaciones cruzadas para garantizar que los modelos no estén sobreajustados a datos históricos específicos.

Finalmente, cualquiera sea la mejora que se pruebe, sin duda concluimos que en esta serie que tiene una alta volatilidad y susceptibilidad a variables del entorno, es importante simplificar la Serie Temporal, limitando el horizonte de datos a un periodo más reciente (por ejemplo, últimos 5 años) para enfocar el análisis en las condiciones económicas actuales y reducir la sensibilidad a ruidos históricos o dinámicas económicas que ya no son relevantes.



